{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb50142",
   "metadata": {},
   "source": [
    "<h2><center>ECE 594N: Homework 4 Group Project</center></h2>\n",
    "<h1><center>Human Pose Classifcation Using Manifold Learning</center></h1>\n",
    "<h3><center>by Abhijith Atreya, Jax Burd, and Christos Zangos</center></h3>\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fd6b3",
   "metadata": {},
   "source": [
    "# Project Goal\n",
    "\n",
    "To distinguish between certain actions using 2D full body-based keypoints pulled from an open dataset. We first performed 2 PCA methods to extract a minimum number of components to represent our data. Both of which we apply logistic regression and compare classifcation accuracies for various number of components chosen.\n",
    "\n",
    "![](images/human_poses.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f6e62",
   "metadata": {},
   "source": [
    "# MPII Human Pose Dataset\n",
    "- <b>Dataset available at:</b> <http://human-pose.mpi-inf.mpg.de/>\n",
    "\n",
    "\n",
    "- <b>Citation:</b> <em>Mykhaylo Andriluka and Leonid Pishchulin and Peter Gehler and Schiele, Bernt. 2D Human Pose Estimation: New Benchmark and State of the Art Analysis. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jun 2014</em>\n",
    "\n",
    "Created as a new benchmark for 2D human pose estimation in 2014, MPII Human pose features about 25K images total, and over 40K people with annotated body joints. The images and their annotations depict 410 different human activities. Some examples are (but not limited to) dancing, running, playing a musical intrument, meditating, etc.\n",
    "\n",
    "![](images/human_pose_activities.PNG)\n",
    "\n",
    "All images were extracted from YouTube, and was the result of a collaboration from researches at the Max Planck Institute for Intelligent Systems in Germany and Stanford University in the United States.\n",
    "\n",
    "### Keypoint Labels\n",
    "The dataset annotated all the joints on their images with the 16 total features listed below, each containing a x and y position coordinate. The dataset also tells us whether or not the joint is visible, as some images don't show the who human body or people are angled in such a way that a keypoint is hidden.\n",
    "\n",
    "![](images/skeleton_diagram.PNG)\n",
    "\n",
    "\n",
    "\n",
    "### Running and Cycling\n",
    "In our project we wanted to focus on two activities within the dataset that would be reasonable to distinguish based off their keypoint skeletons. For this reason we decided to attempt classifying between the activities of <b>running</b> and <b>cycling</b>, as it was assumed their poses are distinct enough to yeild good results.\n",
    "\n",
    "\n",
    "![](images/keypoints.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c498f8",
   "metadata": {},
   "source": [
    "# Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d6f0b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Using numpy backend\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from geomstats.geometry.euclidean import EuclideanMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc3636d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MPII dataset\n",
    "mat = sio.loadmat('mpii_human_pose_v1_u12_2/mpii_human_pose_v1_u12_1.mat', struct_as_record=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81bf5227",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rel = mat['RELEASE']\n",
    "\n",
    "obj_rel = rel[0,0]\n",
    "\n",
    "annolist = obj_rel.__dict__['annolist']\n",
    "img_tra = obj_rel.__dict__['img_train']\n",
    "act = obj_rel.__dict__['act']\n",
    "\n",
    "n = annolist.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a85d652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the columns of the data \n",
    "\n",
    "data_arr = ['NAME','r ankle_X','r ankle_Y', 'r knee_X','r knee_Y', 'r hip_X','r hip_Y', 'l hip_X','l hip_Y', 'l knee_X','l knee_Y', 'l ankle_X','l ankle_Y','pelvis_X','pelvis_Y','thorax_X','thorax_Y','upper neck_X','upper neck_Y', 'head top_X','head top_Y', 'r wrist_X','r wrist_Y','r elbow_X','r elbow_Y', 'r shoulder_X','r shoulder_Y','l shoulder_X','l shoulder_Y','l elbow_X','l elbow_Y', 'l wrist_X','l wrist_Y','Scale','Activity','Category']\n",
    "\n",
    "data = pd.DataFrame(columns=data_arr)\n",
    "\n",
    "Convert_data_to_CSV(annolist, act, data_arr, img_tra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b82c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV\n",
    "df_obj = pd.read_csv('mpii_dataset.csv')\n",
    "input_running_filtered, input_cycling_filtered = Clean_data(df_obj,data_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07345d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an image with keypoints\n",
    "# \"image_id\" is used to index an image from the dataframe\n",
    "image_id = 15\n",
    "imag(input_cycling_filtered,input_cycling_filtered.index[image_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97ef9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset and labels \n",
    "\n",
    "running, r_labels=format_dataframe(input_running_filtered,0)\n",
    "cycling, c_labels=format_dataframe(input_cycling_filtered,1)\n",
    "\n",
    "dataset=np.concatenate((running,cycling))\n",
    "labels=np.concatenate((r_labels,c_labels))\n",
    "dataset[np.isnan(dataset).any(axis=2)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f966a65",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "\n",
    "\n",
    "## On Euclidean Space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c86235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "\n",
    "for i in range(10):\n",
    "    train_acc, test_acc , pca = Euclidian_LR(dataset,labels)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Number of principal components\", size=14)\n",
    "plt.ylabel(\"Fraction of explained variance\", size=14);\n",
    "plt.title(\"Explained variance for Euclidian space\")\n",
    "\n",
    "print(\"Average Training accuracy: \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy: \" + str(np.mean(test_acc_avg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5e277d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection of First X Pricinple Components\n",
    "\n",
    "label_to_str = {0: \"Running\", 1: \"Cycling\"}\n",
    "label_to_color = {\n",
    "    0: (102 / 255, 178 / 255, 255 / 255, 1.0),\n",
    "    1: (255 / 255, 178 / 255, 102 / 255, 1.0),\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for label, col in label_to_color.items():\n",
    "    mask = labels == label\n",
    "    plt.scatter(dataset[mask, 0], dataset[mask, 1], color=col, s=100, label=label_to_str[label])\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\n",
    "    \"Projection on the 2 first principal components\"\n",
    "    \"\\nof PCA in Euclidean shape space\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd81416",
   "metadata": {},
   "source": [
    "## On Kendall Shape Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daf4a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tangent PCA\n",
    "# Perform classification on tangent space\n",
    "\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "\n",
    "for i in range(10):\n",
    "    t_pca, X , train_acc, test_acc  = Geometric_LR(dataset,labels)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t_pca.explained_variance_ratio_)\n",
    "plt.xlabel(\"Number of principal components\", size=14)\n",
    "plt.ylabel(\"Fraction of explained variance\", size=14);\n",
    "plt.title(\"Explained variance for Kendall shape space\")\n",
    "\n",
    "print(\"Average Training accuracy: \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy: \" + str(np.mean(test_acc_avg)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c41daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection of First X Pricinple Tangent Components\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for label, col in label_to_color.items():\n",
    "    mask = labels == label\n",
    "    plt.scatter(X[mask, 0], X[mask, 1], color=col, s=100, label=label_to_str[label])\n",
    "plt.legend(fontsize=14)\n",
    "plt.title(\n",
    "    \"Projection on the 2 first principal components\"\n",
    "    \"\\nof tangent PCA in Kendall shape space\"\n",
    ");\n",
    "\n",
    "#assert feature_names[3][:-1]==feature_names[4][:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b06d173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distance matrix \n",
    "\n",
    "m_ambient = 2\n",
    "k_landmarks = 16\n",
    "\n",
    "preshape = PreShapeSpace(m_ambient=m_ambient, k_landmarks=k_landmarks)\n",
    "matrices_metric = preshape.embedding_metric\n",
    "pose_preshape = preshape.projection(dataset)\n",
    "base_point = pose_preshape[0]\n",
    "pose_shape = preshape.align(point=pose_preshape, base_point=base_point)\n",
    "kendall_metric = KendallShapeMetric(m_ambient=m_ambient, k_landmarks=k_landmarks)\n",
    "dist_pairwise = kendall_metric.dist_pairwise(pose_shape)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "\n",
    "ax = fig.add_subplot(121)\n",
    "\n",
    "eucl_metric = EuclideanMetric(2 * 16)\n",
    "nsamples, nx, ny = dataset.shape\n",
    "posture = dataset.reshape((nsamples,nx*ny))\n",
    "eucl_pair_dist = eucl_metric.dist_pairwise(posture)\n",
    "ax.imshow(eucl_pair_dist);\n",
    "plt.title(\"Distance matrix for Euclidian shape space\")\n",
    "\n",
    "\n",
    "ax = fig.add_subplot(122)\n",
    "ax.imshow(dist_pairwise);\n",
    "plt.title(\"Distance matrix for Kendall shape space\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28d14c4",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3660841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 Principle Components\n",
    "# PCA\n",
    "\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "ncomponenets = 2\n",
    "for i in range(10):\n",
    "    train_acc, test_acc , pca = Euclidian_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "print(\"Average Training accuracy (Eucidian metric): \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy (Eucidian metric): \" + str(np.mean(test_acc_avg)))\n",
    "\n",
    "# Train PCA\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "n_components = 2\n",
    "for i in range(10):\n",
    "    t_pca, X , train_acc, test_acc ,sizes = Geometric_LR(dataset,labels, n_components)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "\n",
    "print(\"Average Training accuracy: \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy: \" + str(np.mean(test_acc_avg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c899274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 Principle Components\n",
    "\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "ncomponenets = 5\n",
    "for i in range(10):\n",
    "    train_acc, test_acc , pca = Euclidian_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "print(\"Average Training accuracy (Eucidian metric): \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy (Eucidian metric): \" + str(np.mean(test_acc_avg)))\n",
    "\n",
    "# Tangent PCA\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "for i in range(10):\n",
    "    t_pca, X , train_acc, test_acc ,sizes = Geometric_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "\n",
    "print(\"Average Training accuracy: \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy: \" + str(np.mean(test_acc_avg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08ab18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10 Principle Components\n",
    "\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "ncomponenets = 10\n",
    "for i in range(10):\n",
    "    train_acc, test_acc , pca = Euclidian_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "print(\"Average Training accuracy (Eucidian metric): \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy (Eucidian metric): \" + str(np.mean(test_acc_avg)))\n",
    "\n",
    "#Tanget PCA\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "for i in range(10):\n",
    "    t_pca, X , train_acc, test_acc ,sizes = Geometric_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "\n",
    "print(\"Average Training accuracy: \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy: \" + str(np.mean(test_acc_avg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f6c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 Principle Components\n",
    "\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "ncomponenets = 20\n",
    "for i in range(10):\n",
    "    train_acc, test_acc , pca = Euclidian_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "print(\"Average Training accuracy (Eucidian metric): \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy (Eucidian metric): \" + str(np.mean(test_acc_avg)))\n",
    "\n",
    "#Tangent PCA\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "for i in range(10):\n",
    "    t_pca, X , train_acc, test_acc ,sizes = Geometric_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "\n",
    "print(\"Average Training accuracy: \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy: \" + str(np.mean(test_acc_avg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a0f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 Components (All)\n",
    "\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "ncomponenets = 32\n",
    "for i in range(10):\n",
    "    train_acc, test_acc , pca = Euclidian_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "print(\"Average Training accuracy (Eucidian metric): \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy (Eucidian metric): \" + str(np.mean(test_acc_avg)))\n",
    "\n",
    "# Tangent PCA\n",
    "train_acc_avg = list()\n",
    "test_acc_avg = list()\n",
    "for i in range(10):\n",
    "    t_pca, X , train_acc, test_acc ,sizes = Geometric_LR(dataset,labels, ncomponenets)\n",
    "    train_acc_avg.append(train_acc)\n",
    "    test_acc_avg.append(test_acc)\n",
    "\n",
    "print(\"Average Training accuracy: \" + str(np.mean(train_acc_avg)))\n",
    "print(\"Average Test accuracy: \" + str(np.mean(test_acc_avg)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282410fc",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "#### Limitations of the Dataset\n",
    "- Keypoints are only 2D\n",
    "- Keypoints may leave out key parts of the body, like individual facial features, fingers, and toes\n",
    "- Not all data is full body and don't feature all 16 keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9ca61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
