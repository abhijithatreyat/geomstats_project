{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb50142",
   "metadata": {},
   "source": [
    "<h2><center>ECE 594N: Homework 4 Group Project</center></h2>\n",
    "<h1><center>Human Pose Classifcation Using Manifold Learning</center></h1>\n",
    "<h3><center>by Abhijith Atreya, Jax Burd, and Christos Zangos</center></h3>\n",
    "    \n",
    "---------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97fd6b3",
   "metadata": {},
   "source": [
    "# Project Goal\n",
    "\n",
    "To distinguish between certain actions using 2D full body-based keypoints pulled from an open dataset. We first performed principle component anaylsis to extract a minimum number of PCs to represent our data, which is then fed into two classification methods for comparison. First is using a distance metric on a tangent space, and the second is the classical ML technique of logistic regression.\n",
    "\n",
    "![](images/human_poses.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f6e62",
   "metadata": {},
   "source": [
    "# MPII Human Pose Dataset\n",
    "- <b>Dataset available at:</b> <http://human-pose.mpi-inf.mpg.de/>\n",
    "\n",
    "\n",
    "- <b>Citation:</b> <em>Mykhaylo Andriluka and Leonid Pishchulin and Peter Gehler and Schiele, Bernt. 2D Human Pose Estimation: New Benchmark and State of the Art Analysis. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jun 2014</em>\n",
    "\n",
    "Created as a new benchmark for 2D human pose estimation in 2014, MPII Human pose features about 25K images total, and over 40K people with annotated body joints. The images and their annotations depict 410 different human activities. Some examples are (but not limited to) dancing, running, playing a musical intrument, meditating, etc.\n",
    "\n",
    "![](images/human_pose_activities.PNG)\n",
    "\n",
    "All images were extracted from YouTube, and was the result of a collaboration from researches at the Max Planck Institute for Intelligent Systems in Germany and Stanford University in the United States.\n",
    "\n",
    "### Keypoint Labels\n",
    "The dataset annotated all of joints on their images with the 16 total features listed below, each containing a x and y position coordinate.\n",
    "\n",
    "| Legs \t| ID \t| Head + Torso \t| ID \t| Arms \t| ID \t|\n",
    "|:---:\t|:---:\t|:---:\t|:---:\t|:---:\t|:---:\t|\n",
    "| right ankle \t| 0 \t| pelvis \t| 6 \t| right wrist \t| 10 \t|\n",
    "| right knee \t| 1 \t| thorax \t| 7 \t| right elbow \t| 11 \t|\n",
    "| right hip \t| 2 \t| neck \t| 8 \t| right shoulder \t| 12 \t|\n",
    "| left hip \t| 3 \t| head \t| 9 \t| left shoulder \t| 13 \t|\n",
    "| left knee \t| 4 \t|  \t|  \t| left elbow \t| 14 \t|\n",
    "| left ankle \t| 5 \t|  \t|  \t| left wrist \t| 15 \t|\n",
    "\n",
    "<h2><center>Insert Keypoint Skeleton Here</center></h2>\n",
    "\n",
    "\n",
    "### Running and Cycling\n",
    "In our project we wanted to focus on two activities within the dataset that would be reasonable to distinguish based off their keypoint skeletons. For this reason we decided to attempt classifying between the activities of <b>running</b> and <b>cycling</b>, as it was assumed their poses are distinct enough to yeild good results.\n",
    "\n",
    "<h2><center>Insert Images of Cycling and Biking Here</center></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f966a65",
   "metadata": {},
   "source": [
    "# Principle Component Analysis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c86235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Christos's Code Below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8fe97",
   "metadata": {},
   "source": [
    "# Classifcation Using Tangent Space Metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a807c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jax's Code Below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5028df",
   "metadata": {},
   "source": [
    "# Classifcation Using Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe4de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abhijith's Code Below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282410fc",
   "metadata": {},
   "source": [
    "# Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f9ca61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
